{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How I write code now.\n",
    "\n",
    "This is a shallow overview of how I write code nowadays. I've come to work this way partly due to what I learned in my classes and partly because I ignored exactly what my classes taught me. I'm not an expert, and I easily get confused about how python interacts with other programs like the terminal, Anaconda, and the OS (Windows10). Also, this is the first time I've used a Jupyter Notebook for this kind of thing, so expect some breakage.\n",
    "\n",
    "## How I used to do it\n",
    "\n",
    "I dug around old files of mine and found this example. This is the whole script, written in python 2."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "def base_amount(base,text):\n",
    "    matchbases=re.findall(base, text, re.I)\n",
    "    return len(matchbases)\n",
    "\n",
    "sequence=open('rosalind_dna.txt').read() \n",
    "base_a=base_amount('a', sequence)\n",
    "base_c=base_amount('c', sequence)\n",
    "base_g=base_amount('g', sequence)\n",
    "base_t=base_amount('t', sequence)\n",
    "\n",
    "print base_a, base_c, base_g, base_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's wrong with it? A few things.\n",
    "* input file is hard coded into the script\n",
    "* does not check what kind of data the text file is\n",
    "* I've forgotten why I wrote this, and there's no documentation to give me any clues. No comment, no docstrings, nothing.\n",
    "* results are unceremoniously dumped to the command line\n",
    "* With respect to style, and the amount of work being done here, I don't need a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I was trying to do\n",
    "\n",
    "My thesis is to describe how well different subsequences of a gene sequence can identify a bacterial species in the human bladder. For the most part, the gene is the 16S ribosomal gene _rrn_, but there are other genes that can be used. I'm using the _rrn_ gene in this example. Any subsequences can be used, but the only practical ones are the result of a polymerase chain reaction (PCR) using different primers that anneal to the template. When you look at the _rrn_ sequence of a collection of bacterial species, the subsequences are not in the same place due to the different evolutionary history of the species. Insertions and deletions move the location of the different species' _rrn_ subsequences relative to each other. \n",
    "\n",
    "A really important step is to locate where the same subsequences are across those species of interest, and extract them. First, I'll create a multisequence alignment (MSA) of the _rrn_ gene, which ensures that the parts of the _rrn_ gene that are the same across the set of species are in the same location. \n",
    "\n",
    "![](pictures/msa.png)\n",
    "\n",
    "This is a screen capture of the program UGENE, which I use because it was the easiest to figure out, but mostly because it's free. In the thin strip between the alignment window and the species names is the position on each respective _rrn_ gene. It's clear (if you squint at the little numbers at the top) that while the position of the MSA is 1286 base pairs from the beginning, the position in each species' _rrn_ sequence is different. Some positions are almost 100bp different from others.\n",
    "\n",
    "![](pictures/mag.png)\n",
    "\n",
    "In the next step, I map the location of the primers to the MSA.\n",
    "\n",
    "![](pictures/amps.png)\n",
    "\n",
    "Finally, I'll extract the subsequence bracketed by each primer from each respecive species' _rrn_ sequence. \n",
    "\n",
    "While I've been showing pictures to illustrate these examples, in reality they're just text files formatted in a particular way. All these steps have been operations on strings of characters. The trouble begins when I try to find the position of the base pair in the MSA to the corresponding position in the individual _rrn_ sequences of each species. I did this by hand, at first, by using the _rrn_ seqeunce of _E. coli_ as a reference for the primer locations, and transposing to the MSA coordinate by scrolling across the display, and then feeding that into a program function from the BioPython module. It's better when a computer does all the steps.\n",
    "\n",
    "Extracting the sequences from a MSA is easy. A BioPyton module `AlignIO` will subset a MSA just like you would use slice notation on a string:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "msa[:, start_pos:end_pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to it. `msa` is the multisequence alignment file object created by `AlignIO`. The rows and columns of the MSA are seperated by a comma. The start and stop are indicated by numbers (or variables set to numbers). So the code above means use all the rows in the `msa`, but extract the column positions from `start_pos` to `end_pos`. The entire python script could just be this one line, but that's not good enough. The remainder of this presentation is about how I explain how to describe what the script does (documentation), what resources it needs (command line arguments), and how I make sure the script does what it's supposed to (unit tests), even when someone else uses it and tries to pass all kinds of garbage to it. Because fixing bugs through code inspection is like crawing through fiberglass insulation in a swimsuit, even when you wrote the code yourself. Even a small set of unit tests and some documentation makes it easier to maintain your code, and easier for someone else to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests\n",
    "\n",
    "I found writing tests is a habit that must be reinforced. Test driven development is not as much fun as getting the code to work, and the number of test posibilities are open ended. I have many times when I cheat, and just bash out some code because I make excuses about how hard it is to think of relevant tests. However, once it's done I get code that has I know will work for the easy exceptions that I can think of, and any new bugs can be added to the test suite. For this script, I was all ready to write the unit tests first. Up to now, I've only done tests on functions, but this script didn't have any. I had to learn how to use the `subprocess` module first, which is a way to call a process (like the shell command `ls`, or even NCBI's `blastn`) from inside a python script. So I worked on validating the input files first. \n",
    "\n",
    "However, I can show how to do a small test suite on a silly function. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def hello_world():\n",
    "    \"\"\"\n",
    "        Just like the name suggests, this\n",
    "        trusty function will print \n",
    "        'Hello World!' and then sit quietly\n",
    "        with its hands in its lap.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modules used\n",
    "\n",
    "I'm going to import all the modules that I use in this notebook here at the beginning.\n",
    "\n",
    "* [unittest](https://docs.python.org/3/library/unittest.html) - supports test automation, sharing of setup and shutdown code for tests, aggregation of tests into collections, and independence of the tests from the reporting framework.\n",
    "* [argparse](https://docs.python.org/3/library/argparse.html?highlight=argparse#module-argparse) - this module makes it easy to write user-friendly command-line interfaces.\n",
    "* [Biopython](https://biopython.org/) - a distributed collaborative effort to develop Python libraries and applications which address the needs of current and future work in bioinformatics.\n",
    "* [io](https://docs.python.org/3/library/io.html?highlight=io#module-io) - This module provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available.\n",
    "* [sys](https://docs.python.org/3/library/sys.html?highlight=sys#module-sys) - \n",
    "* [os](https://docs.python.org/3/library/os.html?highlight=os#module-os) - This module provides a portable way of using operating system dependent functionality.\n",
    "* [datetime](https://docs.python.org/3/library/datetime.html?highlight=datetime#module-datetime) - this module supplies classes for manipulating dates and times.\n",
    "* [shutil](https://docs.python.org/3/library/shutil.html?highlight=shutil#module-shutil) - This module offers a number of high-level operations on files and collections of files. In particular, functions are provided which support file copying and removal.\n",
    "* [subprocess](https://docs.python.org/3/library/subprocess.html?highlight=subprocess#module-subprocess) - This module allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import argparse\n",
    "from Bio import SeqIO\n",
    "from Bio import AlignIO\n",
    "import io\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "# this is a script that I wrote\n",
    "import extra_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing Hello World\n",
    "\n",
    "Tests are grouped into a single class. When the test script is run, `unittest` will look for anything starting with the word 'test' and run it automatically. So far I've written tests for each collection of functions that I use in my workflow. An easy test I like to include first is to check that the file is where I think it is. It's a stupid test, but I move files around sometimes. A failed test might be because it can't find the file in the first place, not because the tested code is at fault. The next test is to check that the function has actually printed \"Hello World!\" to the standard output, which is the terminal window. It's a good example of how the desire to do something simple turns into an hour of discovering how many layers are between the code I write and where the output ends up. But now I know, so in the future I can apply it to some other test.\n",
    "\n",
    "The bulk of the tests I've written are based on assertions. Assert that these two values are equal. Assert that this boolean is 'True'. Assert that the file exists. That kind of thing. If you do nothing else with unit tests, assertions will still take you far.\n",
    "\n",
    "With test driven development, you're supposed to start with a piece of paper or whiteboard. Then you write down what the code is supposed to accomplish, and with what resources (arguments, files, data, etc.). Then you conceive of a bunch of tests that will verify that the code will do that. Then you write the code. Once the code passes all the test you have written, you _stop writing code_. Back away from the keyboard, you're all done. \n",
    "\n",
    "I have yet to do this. I do start with paper, and fill it with boxes and arrows, and then write some tests. Then I'll write the code, and run the test. By then, I'll have realized that I need to add some other extra feature. So I'll put that in, and write a test for it. Rinse and repeat. The improvement over what I used to do is that the code still does what I had originally intended, even after extensive refactoring (rewriting code that's more concise is called 'refactoring'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_output (__main__.test_simple) ... ok\n",
      "test_script_exists (__main__.test_simple) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1b78a6081d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class test_simple(unittest.TestCase):\n",
    "    \"\"\"\n",
    "        A test suite\n",
    "        \n",
    "        All test functions must start with 'test' in order \n",
    "        for unittest to automatically find and run \n",
    "    \"\"\"\n",
    "    def test_script_exists(self):\n",
    "        \"\"\"\n",
    "            make sure you can find the file\n",
    "        \"\"\"\n",
    "        get_files=os.listdir(\"./\")\n",
    "        self.assertIn(\"extra_functions.py\", get_files)\n",
    "        \n",
    "    def test_output(self):\n",
    "        \"\"\"\n",
    "            checking the printed output was ridiculously difficult to do. \n",
    "            Thanks alot, python.\n",
    "        \"\"\"\n",
    "        # create a stringIO object\n",
    "        catch=io.StringIO()\n",
    "        # redirect the sys.stdout\n",
    "        sys.stdout=catch\n",
    "        # call the function\n",
    "        extra_functions.hello_world()\n",
    "        # put the stdout back where it's supposed to be\n",
    "        sys.stdout=sys.__stdout__\n",
    "        #print(catch.getvalue())\n",
    "        # assert that the caught output is what it's supposed to be\n",
    "        self.assertEqual(catch.getvalue().strip(), 'Hello World!')\n",
    "\n",
    "# this is how I got unittest to run inside the notebook\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that I put a lot of comments in `test_output()`, but none in `test_script_exists()`. If I'm doing something new, like redirecting where the output is going, then I'll put as many comments as needed so that my future self can figure out what I was doing. If I think it's already clear, like in the two lines of `test_script_exists()`, I won't. I've also gotten in the habit of writing a decent docstring, which is everything between the triple quotes. All that text can be displayed with a call to the `__doc__` method. I can't get it to format correctly in this notebook, but if you hit `shift+tab` a window will popup and display the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n        Just like the name suggests, this\\n        trusty function will print \\n        'Hello World!' and then sit quietly\\n        with its hands in its lap.\\n    \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_functions.hello_world.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation and command line flags with `argparse`\n",
    "\n",
    "I've found it's a good idea to write out as much of the documentation early, such as in a docstring, but many times I didn't see a point because it was hard to show all that documentation where it was needed - at the command line, where I was getting stuck. I would have to scroll through the code or worse, follow a trail to the script in some other directory. Then I stumbled across the `argparse` module. I really like `argparse`. Like a shell program such as `grep`, `argparse` lets you tell the script to use a particular file, with a particular context, and then send the output to whatever directory you want. I used to do this in a small way by using `sys.argv` from the `sys` module, but I think `argparse` is much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script and function documentation\n",
    "\n",
    "The basic format that I've settled on is to write a detailed description of what the script does in the `description` argument, then add as many optional or positional arguments as needed. Setting the argument `formatter_class=argparse.RawDescriptionHelpFormatter` allows me to format the text as I want it to display, which is easier than fussing with `\\t` or `\\n` characters. Then I list all the functions that I wrote that the script needs in the `epilog` argument. I don't know how to set the formatting in this part, so I use the `format` method of the string object. See the `extra_functions.hello_world.__doc__`? You might have to scroll to the right. That's where calling the docstring in a function comes in handy. \n",
    "\n",
    "Showing the documentation is done by adding a `--help` or `-h` flag after calling the script:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">python your_script.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carter\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parsing=argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description=\"\"\"\n",
    "         A long and detailed description of what this script will do. \n",
    "         And an example of how to run it from the command line, \n",
    "         because you know you'll forget.\n",
    "         \n",
    "             >python dummy.py -i /path/to/foo.txt\n",
    "\"\"\", epilog=\"I like to list the docstrings of functions that I wrote myself here,\\nespecially ones that live somewhere else in the directory tree:\\n\\n  hello_world()\\n{}\".format(extra_functions.hello_world.__doc__))\n",
    "\n",
    "parsing.add_argument(\"--infile\", \"-i\", required=True, help=\"path to the infile\")\n",
    "parsing.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"spell out what is happening\")\n",
    "\n",
    "# this is how I got argparse to work in the notebook, without calling a script\n",
    "# in a script, I would type:\n",
    "#    args=parsing.parse_args()\n",
    "\n",
    "args=parsing.parse_args(['-h'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options\n",
    "\n",
    "You get a `--help` option for free, but it's up to you to write something helpful. Anything beyond that is added by typing another `parsing.add_argument(args)` line. What you call this new option is entirely up to you. `--mike_wazowski` is fine. So is `--blank`. There are many arguments that you can set in the `.add_argument` method, and I have yet to use them all. Some examples are default values, whether the option is required, and what data type the option is. The most important in this context is the `help` argument. This is where you describe what the function of that option is. \n",
    "\n",
    "I commonly use three options on a regular basis.\n",
    "\n",
    "* `--infile` or `--infolder` The path to the file that the script will work on, or the folder where a batch of files will be used\n",
    "\n",
    "* `--outfile` The path to where I want output written to\n",
    "\n",
    "* `--verbose` I tend to use a lot of `print` statements when I develop code. Instead of going back and deleting them, or commenting them out, I put them in an `if` statement and check if the `--verbose` option is used. Like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if args.verbose:\n",
    "    print(\"whatever I was interested in at this point in the code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of running the cell above doesn't show up in the notebook (and it seems to throw an error), but if you look at the terminal where Jupyter is running, you should see this:\n",
    "\n",
    "![](pictures/terminal_output.png)\n",
    "\n",
    "Here's an example of one script where I went all in with the `--help` documentation, including a lush ASCII diagram.\n",
    "\n",
    "![](pictures/long_term.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate data with `try...except` clauses \n",
    "\n",
    "I need the script to extract parts of a MSA pretty much exactly where I tell it to. Upstream work has generated files using the Clustal format, and I tend to belive that it's done right. But, I might have to write data in a Clustal format on my own in the future, and that means I'll probably get it wrong. I use `try...except` clauses to validate the format, but I have to admit that for now I'm depending on BioPython to do the validating.\n",
    "\n",
    "The `try...except` clause is a method of gracefully handling errors that you suspect might happen. Instead of the python interpreter killing the whole script and printing an error message, the clause allows you to tell the interpreter what to do next. Maybe, it's nothing more than writing your own custom error output. Maybe it's only going to affect one variable, and you can set it to something else. In general, the rule is that if your script is receiving data from somewhere else, it should absolutely positively make sure that the data is what is expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made a few Clustal files for the unit tests and in-script validation. First was a correctly formatted clustal file:\n",
    "\n",
    "#### good_clustal_format.aln"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CLUSTAL mock file\n",
    "\n",
    "seq1                     GGGGGGGGTTTTTT----GG\n",
    "CP016007.3589827.3591382 GGG---GGTTTTTTTTTGGG\n",
    "seq3                     GGGGGGGGTT------TGGG\n",
    "seq4                     -----GGGTTTTTTTTTGGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an incorrectly formatted file, where just the header is not to spec:\n",
    "\n",
    "#### bad_clustal_format.aln"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mock file\n",
    "\n",
    "seq1                     GGGGGGGGTTTTTT----GG\n",
    "CP016007.3589827.3591382 GGG---GGTTTTTTTTTGGG\n",
    "seq3                     GGGGGGGGTT------TGGG\n",
    "seq4                     -----GGGTTTTTTTTTGGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what happens when BioPython comes across a malformed Clustal file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm using the infile option (-i) and a path to the badly formatted Clustal file\n",
    "args=parsing.parse_args(['-i', 'bad_clustal_format.aln'])\n",
    "msa = AlignIO.read(args.infile, \"clustal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AlignIO` will expect a vaild Clustal file, and will throw an error with a lot of text when it comes across anything else. The type is a `ValueError`, and I'll look for that error type in a `try...except` clause, and later in an assertion test when I run the test suite.\n",
    "\n",
    "In the code below, nothing happens because BioPython agrees that the format of the file meets the Clustal specs. I've also introduced a `try...except` clause to handle the error thrown by `AlignIO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=parsing.parse_args(['-i', 'good_clustal_format.aln'])\n",
    "\n",
    "try:\n",
    "    msa = AlignIO.read(args.infile, \"clustal\")\n",
    "except ValueError or AssertionError:\n",
    "    print(\"\\n\\tThe format of the file \\\"{}\\\" doesn't meet the Clustal file specifications:\\n\\n\\t1) The first line in the file must start with the words \\\"CLUSTAL W\\\" or \\\"CLUSTALW\\\". Other information\\n\\t   in the first line is ignored.\\n\\t2) One or more empty lines.\\n\\t3) One or more blocks of sequence data. Each block consists of:\\n\\t\\t* One line for each sequence in the alignment. Each line consists of:\\n\\t\\t\\t* the sequence name\\n\\t\\t\\t* white space\\n\\t\\t\\t* up to 60 sequence symbols.\\n\\t\\t\\t* optional - white space followed by a cumulative count of residues for the sequences\\n\\t\\t* A line showing the degree of conservation for the columns of the alignment in this block\\n\\t\\t (this can be left as blank. \\\" \\\" indicates no match, but for this application\\n\\t\\t the conservation isn't important)\\n\\t\\t* One or more empty lines.\\n\\n\\tQuitting now.\".format(args.infile))\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the python interpreter will try to execute the code in the `try` portion of the clause. If all goes well, the interpreter exits the clause and continues with the next block of code. If an error does occur in the `try` portion, the interpreter moves to the `except` portion and executes the code there. Most importantly, it will attempt to continue on to the next code block, regardless of what error has happened (unless that error includes exiting the script). This is why the clause is usefull. Errors that would normally stop the script can be circumvented. The downside is that the clause will suppress any error messages, so I need some other way to signal the test suite that some error has happened. `sys.exit()` is a good way to do this.\n",
    "\n",
    "In the `try` portion, I attempt to have `AlignIO` read in a Clustal file. If things are fine, the interpreter moves on to the next code block. If not, I have the interpreter look at the type of error `AlignIO` has thrown. If it's a `ValueError` or an `AssertionError`, the `sys` module will force an exit from the script and send a return code of 1. The convention of what number to use for the status of a program when it terminates is listed below.\n",
    "\n",
    "* Exit status 0 = successfully run, all is well\n",
    "* Exit status 1 = failure, as defined by the program\n",
    "* Exit status 2 = command line useage failure\n",
    "\n",
    "I want this to happen. I'm using the clause to print an error message I wrote myself explaining that the file is no good, and I don't want the script to continue to the next code block. I could have defined a custom message for these errors, but I'm using the value of the exit code for the test suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a malformed file will trigger the error message and exit the script. Plus you should be able to see that the `SystemExit` is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args=parsing.parse_args(['-i', 'bad_clustal_format.aln'])\n",
    "\n",
    "try:\n",
    "    msa = AlignIO.read(args.infile, \"clustal\")\n",
    "except ValueError or AssertionError:\n",
    "    print(\"\\n\\tThe format of the file \\\"{}\\\" doesn't meet the Clustal file specifications:\\n\\n\\t1) The first line in the file must start with the words \\\"CLUSTAL W\\\" or \\\"CLUSTALW\\\". Other information\\n\\t   in the first line is ignored.\\n\\t2) One or more empty lines.\\n\\t3) One or more blocks of sequence data. Each block consists of:\\n\\t\\t* One line for each sequence in the alignment. Each line consists of:\\n\\t\\t\\t* the sequence name\\n\\t\\t\\t* white space\\n\\t\\t\\t* up to 60 sequence symbols.\\n\\t\\t\\t* optional - white space followed by a cumulative count of residues for the sequences\\n\\t\\t* A line showing the degree of conservation for the columns of the alignment in this block\\n\\t\\t (this can be left as blank. \\\" \\\" indicates no match, but for this application\\n\\t\\t the conservation isn't important)\\n\\t\\t* One or more empty lines.\\n\\n\\tQuitting now.\".format(args.infile))\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the notebook terminal window, you should see this output:\n",
    "\n",
    "![](pictures/exit1.png)\n",
    "\n",
    "This is a win. I put several of these `try...except` clauses throughout my script, and I'll use the exit codes for the test suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unittests for `extract_demo.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this is the full test suite I have for the `extract_demo.py` script. There's one last test file to add, and that's the check for a reference sequence from _E. coli_.\n",
    "\n",
    "#### no_ref_seq.aln"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CLUSTAL mock file\n",
    "\n",
    "seq1                     GGGGGGGGTTTTTT----GG\n",
    "no_reference             GGG---GGTTTTTTTTTGGG\n",
    "seq3                     GGGGGGGGTT------TGGG\n",
    "seq4                     -----GGGTTTTTTTTTGGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_alignio_hates_infile (__main__.test_ambitious) ... ok\n",
      "test_check_output (__main__.test_ambitious) ... ok\n",
      "test_find_resources (__main__.test_ambitious) ... ok\n",
      "test_no_ref_seq (__main__.test_ambitious) ... ok\n",
      "test_script_exists (__main__.test_ambitious) ... ok\n",
      "test_output (__main__.test_simple) ... ok\n",
      "test_script_exists (__main__.test_simple) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.895s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1b78a687c18>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class test_ambitious(unittest.TestCase):\n",
    "    \"\"\"\n",
    "        subprocess.run() is new to me. Call the function and any \n",
    "        options as a list of arguments. \n",
    "\n",
    "        I had a lot of problems at first but figured it out.\n",
    "            \n",
    "        The paths to the different files were confusing. \n",
    "        Settled on using the cwd=\"where/is/foo.bar\" to change the working directory \n",
    "        to the right /src_files dir, then point to the testing file by relative path.\n",
    "            \n",
    "        BioPython got confused when an argument in the subprocess list was like this:\n",
    "            \n",
    "            subprocess.run([\"python\", \"extract_demo.py\", \"-i bad_clustal_format.aln\"]\n",
    "                                                                  ^\n",
    "            \n",
    "        There's something going on in the Windows environment that makes the OS \n",
    "        look for a file named \" foo.bar\", a file with a space as the first character.\n",
    "        Changing that to split up the flag from the path fixed that:\n",
    "            \n",
    "            subprocess.run([\"python\", \"extract_demo.py\", \"-i\", \"bad_clustal_format.aln\"]\n",
    "                                                               ^\n",
    "                    \n",
    "        Now you know.\n",
    "    \"\"\"\n",
    "    def test_script_exists(self):\n",
    "        \"\"\"\n",
    "            check the script exists\n",
    "        \"\"\"\n",
    "        get_files=os.listdir(\"./\")\n",
    "        self.assertIn(\"extract_demo.py\", get_files)\n",
    "        \n",
    "    def test_find_resources(self):\n",
    "                \n",
    "        get_files=os.listdir(\"./\")\n",
    "        self.assertIn(\"good_clustal_format.aln\", get_files)\n",
    "        self.assertIn(\"bad_clustal_format.aln\", get_files)\n",
    "        self.assertIn(\"no_ref_seq.aln\", get_files)\n",
    "        \n",
    "    def test_alignio_hates_infile(self):\n",
    "        \"\"\"\n",
    "            Since you're checking for a return code from sys.exit(), \n",
    "            make sure it's not confused with an exit code of 1 coming from \n",
    "                1) some other complaint raised by subprocess \n",
    "                2) some screwup in the tested script\n",
    "        \"\"\"\n",
    "\n",
    "        checkit=subprocess.run([\"python\", \"extract_demo.py\", \"-i\", \"bad_clustal_format.aln\"], capture_output=True)\n",
    "\n",
    "        # in the future, if you need to search the error message \n",
    "        # for something like 'ValueError:' you can use regex and assertTrue()\n",
    "        # but make sure that the stderr is a string by setting text=True\n",
    "        #value_error=re.compile('ValueError:')\n",
    "        #self.assertTrue(value_error.search(checkit.stderr))\n",
    "        \n",
    "        # try...except will suppress the error output.\n",
    "        # good thing I used a return code\n",
    "        self.assertEqual(checkit.returncode, 1)\n",
    "        \n",
    "    def test_no_ref_seq(self):\n",
    "        \"\"\"\n",
    "            return code should be 1 if there's no E. coli reference sequence. \n",
    "            Either CP016007.2543965.2545520 or CP016007.3589827.3591382\n",
    "        \"\"\"\n",
    "        \n",
    "        checkit=subprocess.run([\"python\", \"extract_demo.py\", \"-i\", \"no_ref_seq.aln\"], capture_output=True)\n",
    "        \n",
    "        self.assertEqual(checkit.returncode, 1)\n",
    "        \n",
    "    def test_check_output(self):\n",
    "        \"\"\"\n",
    "            the output goes to /processed_data, which is nice\n",
    "            \n",
    "            probably not the best way to test, but since the extracted sequences should only be Ts, \n",
    "            then asserting that the set(\"TTTTT\") is equal to the set(\"T\"). Both should be just T.\n",
    "        \"\"\"\n",
    "        \n",
    "        checkit=subprocess.run([\"python\", \"extract_demo.py\", \"-i\", \"good_clustal_format.aln\", \"-e\", \"5,13\"], capture_output=True, text=True)\n",
    "        \n",
    "        print(\"\\n\\targs={}\\n\\treturn code={}\\n\\tstdout={}\\n\\tstderr={}\".format(checkit.args, checkit.returncode, checkit.stdout, checkit.stderr))\n",
    "        \n",
    "        spl_output=checkit.stdout.strip().split(\"/\")\n",
    "        new_file=\"processed_data/{0}/{1}\".format(spl_output[-2], spl_output[-1])\n",
    "        \n",
    "        for x in SeqIO.parse(new_file, \"fasta\"):\n",
    "            self.assertEqual(set(x), set(\"T\"))\n",
    "          \n",
    "        # clean up by removing the directory and all files in it\n",
    "        shutil.rmtree(\"processed_data/{}\".format(spl_output[-2]))\n",
    "        \n",
    "        \n",
    "# this is how I got unittest to run inside the notebook\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
